{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"### Setting up the environment\n","metadata":{}},{"cell_type":"code","source":"!pip install keras_core","metadata":{"execution":{"iopub.status.busy":"2023-09-20T06:24:04.510923Z","iopub.execute_input":"2023-09-20T06:24:04.511565Z","iopub.status.idle":"2023-09-20T06:24:11.480680Z","shell.execute_reply.started":"2023-09-20T06:24:04.511537Z","shell.execute_reply":"2023-09-20T06:24:11.479544Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os\n\nos.environ[\"KERAS_BACKEND\"] = \"jax\"\n\nimport jax\nimport numpy as np\nimport tensorflow as tf\nimport keras_core as keras\n\nfrom jax.experimental import mesh_utils\nfrom jax.sharding import Mesh\nfrom jax.sharding import NamedSharding\nfrom jax.sharding import PartitionSpec as P\n","metadata":{"execution":{"iopub.status.busy":"2023-09-20T06:24:11.482780Z","iopub.execute_input":"2023-09-20T06:24:11.483122Z","iopub.status.idle":"2023-09-20T06:24:25.676944Z","shell.execute_reply.started":"2023-09-20T06:24:11.483085Z","shell.execute_reply":"2023-09-20T06:24:25.675884Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Model Architecture\n\nDefine the model structure using Keras.\n\nDefine any model here, dosent just have to be a normal CNN.\n","metadata":{}},{"cell_type":"code","source":"def get_model():\n    # Make a simple convnet with batch normalization and dropout.\n    inputs = keras.Input(shape=(28, 28, 1))\n    x = keras.layers.Rescaling(1.0 / 255.0)(inputs)\n    x = keras.layers.Conv2D(filters=12, kernel_size=3, padding=\"same\", use_bias=False)(\n        x\n    )\n    x = keras.layers.BatchNormalization(scale=False, center=True)(x)\n    x = keras.layers.ReLU()(x)\n    x = keras.layers.Conv2D(\n        filters=24,\n        kernel_size=6,\n        use_bias=False,\n        strides=2,\n    )(x)\n    x = keras.layers.BatchNormalization(scale=False, center=True)(x)\n    x = keras.layers.ReLU()(x)\n    x = keras.layers.Conv2D(\n        filters=32,\n        kernel_size=6,\n        padding=\"same\",\n        strides=2,\n        name=\"large_k\",\n    )(x)\n    x = keras.layers.BatchNormalization(scale=False, center=True)(x)\n    x = keras.layers.ReLU()(x)\n    x = keras.layers.GlobalAveragePooling2D()(x)\n    x = keras.layers.Dense(256, activation=\"relu\")(x)\n    x = keras.layers.Dropout(0.5)(x)\n    outputs = keras.layers.Dense(10)(x)\n    model = keras.Model(inputs, outputs)\n    return model","metadata":{"execution":{"iopub.status.busy":"2023-09-20T06:24:25.678182Z","iopub.execute_input":"2023-09-20T06:24:25.678666Z","iopub.status.idle":"2023-09-20T06:24:25.690978Z","shell.execute_reply.started":"2023-09-20T06:24:25.678637Z","shell.execute_reply":"2023-09-20T06:24:25.690195Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Data Preprocessing\n\nLoading and processing the MNIST dataset.\n","metadata":{}},{"cell_type":"code","source":"def get_datasets():\n    # Load the data and split it between train and test sets\n    (x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data()\n\n    # Scale images to the [0, 1] range\n    x_train = x_train.astype(\"float32\")\n    x_test = x_test.astype(\"float32\")\n    # Make sure images have shape (28, 28, 1)\n    x_train = np.expand_dims(x_train, -1)\n    x_test = np.expand_dims(x_test, -1)\n    print(\"x_train shape:\", x_train.shape)\n    print(x_train.shape[0], \"train samples\")\n    print(x_test.shape[0], \"test samples\")\n\n    # Create TF Datasets\n    train_data = tf.data.Dataset.from_tensor_slices((x_train, y_train))\n    eval_data = tf.data.Dataset.from_tensor_slices((x_test, y_test))\n    return train_data, eval_data","metadata":{"execution":{"iopub.status.busy":"2023-09-20T06:24:25.693079Z","iopub.execute_input":"2023-09-20T06:24:25.693356Z","iopub.status.idle":"2023-09-20T06:24:25.711741Z","shell.execute_reply.started":"2023-09-20T06:24:25.693331Z","shell.execute_reply":"2023-09-20T06:24:25.710951Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Training Configuration\n","metadata":{}},{"cell_type":"code","source":"# Config\nnum_epochs = 20\nbatch_size = 8096 * 2 # Cause why not\n\ntrain_data, eval_data = get_datasets()\ntrain_data = train_data.batch(batch_size, drop_remainder=True)\n\nmodel = get_model()\noptimizer = keras.optimizers.Adam(1e-3)\nloss = keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n\n(one_batch, one_batch_labels) = next(iter(train_data))\nmodel.build(one_batch)\noptimizer.build(model.trainable_variables)\n","metadata":{"execution":{"iopub.status.busy":"2023-09-20T06:24:25.712665Z","iopub.execute_input":"2023-09-20T06:24:25.712936Z","iopub.status.idle":"2023-09-20T06:24:32.258951Z","shell.execute_reply.started":"2023-09-20T06:24:25.712912Z","shell.execute_reply":"2023-09-20T06:24:32.257879Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Loss & Gradient Computation\n\nJAX computation is purely stateless\nIn JAX, everything must be a stateless function -- so our loss computation function must be stateless as well. That means that all Keras variables (e.g. weight tensors) must be passed as function inputs, and any variable that has been updated during the forward pass must be returned as function output. The function have no side effect.\n\nDuring the forward pass, the non-trainable variables of a Keras model might get updated. These variables could be, for instance, RNG seed state variables or BatchNormalization statistics. We're going to need to return those. So we need something like this:\n","metadata":{}},{"cell_type":"code","source":"# This is the loss function that will be differentiated.\n# Keras provides a pure functional forward pass: model.stateless_call\n\ndef compute_loss(trainable_variables, non_trainable_variables, x, y):\n    \n    y_pred, updated_non_trainable_variables = model.stateless_call(trainable_variables, non_trainable_variables, x)\n    \n    loss_value = loss(y, y_pred)\n    \n    return loss_value, updated_non_trainable_variables\n","metadata":{"execution":{"iopub.status.busy":"2023-09-20T06:24:32.260138Z","iopub.execute_input":"2023-09-20T06:24:32.260434Z","iopub.status.idle":"2023-09-20T06:24:32.265930Z","shell.execute_reply.started":"2023-09-20T06:24:32.260409Z","shell.execute_reply":"2023-09-20T06:24:32.264992Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Once you have such a function, you can get the gradient function by specifying hax_aux in value_and_grad: it tells JAX that the loss computation function returns more outputs than just the loss. Note that the loss should always be the first output.","metadata":{}},{"cell_type":"code","source":"# Function to compute gradients\ncompute_gradients = jax.value_and_grad(compute_loss, has_aux=True)","metadata":{"execution":{"iopub.status.busy":"2023-09-20T06:24:32.267067Z","iopub.execute_input":"2023-09-20T06:24:32.267339Z","iopub.status.idle":"2023-09-20T06:24:32.279495Z","shell.execute_reply.started":"2023-09-20T06:24:32.267314Z","shell.execute_reply":"2023-09-20T06:24:32.278662Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Training step definition\n","metadata":{}},{"cell_type":"markdown","source":"By default, JAX operations run eagerly, just like in TensorFlow eager mode and PyTorch eager mode. And just like TensorFlow eager mode and PyTorch eager mode, it's pretty slow -- eager mode is better used as a debugging environment, not as a way to do any actual work. So let's make our `train_step` fast by compiling it.\n\nWhen you have a stateless JAX function, you can compile it to XLA via the `@jax.jit` decorator. It will get traced during its first execution, and in subsequent executions you will be executing the traced graph (this is just like `@tf.function(jit_compile=True)`. Let's try it:","metadata":{}},{"cell_type":"code","source":"# Training step, Keras provides a pure functional optimizer.stateless_apply\n@jax.jit\ndef train_step(train_state, x, y):\n    trainable_variables, non_trainable_variables, optimizer_variables = train_state\n    (loss_value, non_trainable_variables), grads = compute_gradients(\n        trainable_variables, non_trainable_variables, x, y\n    )\n\n    trainable_variables, optimizer_variables = optimizer.stateless_apply(\n        optimizer_variables, grads, trainable_variables\n    )\n\n    return loss_value, (\n        trainable_variables,\n        non_trainable_variables,\n        optimizer_variables,\n    )\n\n","metadata":{"execution":{"iopub.status.busy":"2023-09-20T06:24:32.280517Z","iopub.execute_input":"2023-09-20T06:24:32.280902Z","iopub.status.idle":"2023-09-20T06:24:32.289723Z","shell.execute_reply.started":"2023-09-20T06:24:32.280875Z","shell.execute_reply":"2023-09-20T06:24:32.288924Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Replicate model and optimizer\n","metadata":{}},{"cell_type":"code","source":"# Replicate the model and optimizer variable on all devices\ndef get_replicated_train_state(devices):\n    # All variables will be replicated on all devices\n    var_mesh = Mesh(devices, axis_names=(\"_\"))\n    # In NamedSharding, axes not mentioned are replicated (all axes here)\n    var_replication = NamedSharding(var_mesh, P())\n\n    # Apply the distribution settings to the model variables\n    trainable_variables = jax.device_put(model.trainable_variables, var_replication)\n    non_trainable_variables = jax.device_put(\n        model.non_trainable_variables, var_replication\n    )\n    optimizer_variables = jax.device_put(optimizer.variables, var_replication)\n\n    # Combine all state in a tuple\n    return (trainable_variables, non_trainable_variables, optimizer_variables)\n","metadata":{"execution":{"iopub.status.busy":"2023-09-20T06:24:32.290716Z","iopub.execute_input":"2023-09-20T06:24:32.290985Z","iopub.status.idle":"2023-09-20T06:24:32.305370Z","shell.execute_reply.started":"2023-09-20T06:24:32.290962Z","shell.execute_reply":"2023-09-20T06:24:32.304490Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"num_devices = len(jax.local_devices())\nprint(f\"Running on {num_devices} devices: {jax.local_devices()}\")\ndevices = mesh_utils.create_device_mesh((num_devices,))","metadata":{"execution":{"iopub.status.busy":"2023-09-20T06:24:32.308143Z","iopub.execute_input":"2023-09-20T06:24:32.308413Z","iopub.status.idle":"2023-09-20T06:24:32.319503Z","shell.execute_reply.started":"2023-09-20T06:24:32.308389Z","shell.execute_reply":"2023-09-20T06:24:32.318694Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Data will be split along the batch axis\ndata_mesh = Mesh(devices, axis_names=(\"batch\",))  # naming axes of the mesh\ndata_sharding = NamedSharding(\n    data_mesh,\n    P(\n        \"batch\",\n    ),\n)  # naming axes of the sharded partition\n\n# Display data sharding\nx, y = next(iter(train_data))\nsharded_x = jax.device_put(x.numpy(), data_sharding)\nprint(\"Data sharding\")\njax.debug.visualize_array_sharding(jax.numpy.reshape(sharded_x, [-1, 28 * 28]))\n","metadata":{"execution":{"iopub.status.busy":"2023-09-20T06:24:32.320509Z","iopub.execute_input":"2023-09-20T06:24:32.320804Z","iopub.status.idle":"2023-09-20T06:24:32.522961Z","shell.execute_reply.started":"2023-09-20T06:24:32.320779Z","shell.execute_reply":"2023-09-20T06:24:32.522011Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Training Loop\n","metadata":{}},{"cell_type":"markdown","source":"The following are the part of train state\n\n``` python\ntrainable_variables = model.trainable_variables\nnon_trainable_variables = model.non_trainable_variables\noptimizer_variables = optimizer.variables\nstate = trainable_variables, non_trainable_variables, optimizer_variables\n```","metadata":{}},{"cell_type":"code","source":"train_state = get_replicated_train_state(devices)","metadata":{"execution":{"iopub.status.busy":"2023-09-20T06:24:32.524113Z","iopub.execute_input":"2023-09-20T06:24:32.524418Z","iopub.status.idle":"2023-09-20T06:24:32.868976Z","shell.execute_reply.started":"2023-09-20T06:24:32.524391Z","shell.execute_reply":"2023-09-20T06:24:32.867719Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import time\n\nprint(f\"Batch Size: {batch_size}\")\n\n# Warm-up epoch\nstart_time = time.time()\ndata_iter = iter(train_data)\nfor data in data_iter:\n    x, y = data\n    sharded_x = jax.device_put(x.numpy(), data_sharding)\n    loss_value, train_state = train_step(train_state, sharded_x, y.numpy())\nend_time = time.time()\nwarmup_time = end_time - start_time\nprint(\"Warmup time:\", warmup_time)\n\n# Training loop\ntotal_time = 0.0\nfor epoch in range(num_epochs):\n    data_iter = iter(train_data)\n    start_time = time.time()\n    for data in data_iter:\n        x, y = data\n        sharded_x = jax.device_put(x.numpy(), data_sharding)\n        loss_value, train_state = train_step(train_state, sharded_x, y.numpy())\n    end_time = time.time()\n    epoch_time = end_time - start_time\n    total_time += epoch_time\n    print(\"Epoch\", epoch, \"loss:\", loss_value)\n\naverage_time_per_epoch = total_time / num_epochs\ntotal_time_excl_warmup = total_time\n\nprint(\"Average time per epoch:\", average_time_per_epoch)\nprint(\"Total time excluding warm-up:\", total_time_excl_warmup)\n","metadata":{"execution":{"iopub.status.busy":"2023-09-20T06:24:32.870225Z","iopub.execute_input":"2023-09-20T06:24:32.870531Z","iopub.status.idle":"2023-09-20T06:24:44.142568Z","shell.execute_reply.started":"2023-09-20T06:24:32.870504Z","shell.execute_reply":"2023-09-20T06:24:44.141095Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Post-Processing\n\nA key thing to notice here is that the loop is entirely stateless -- the variables attached to the model (`model.weights`) are never getting updated during the loop. Their new values are only stored in the state tuple. That means that at some point, before saving the model, you should be attaching the new variable values back to the model.\n\nJust call `variable.assign(new_value)` on each model variable you want to update:\n","metadata":{}},{"cell_type":"code","source":"# Post-processing model state update to write them back into the model\ntrainable_variables, non_trainable_variables, optimizer_variables = train_state\nfor variable, value in zip(model.trainable_variables, trainable_variables):\n    variable.assign(value)\nfor variable, value in zip(model.non_trainable_variables, non_trainable_variables):\n    variable.assign(value)","metadata":{"execution":{"iopub.status.busy":"2023-09-20T06:24:44.143946Z","iopub.execute_input":"2023-09-20T06:24:44.144308Z","iopub.status.idle":"2023-09-20T06:24:44.367421Z","shell.execute_reply.started":"2023-09-20T06:24:44.144278Z","shell.execute_reply":"2023-09-20T06:24:44.366059Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Saving the hardwork","metadata":{}},{"cell_type":"code","source":"model.save('model.keras')","metadata":{"execution":{"iopub.status.busy":"2023-09-20T06:24:44.368748Z","iopub.execute_input":"2023-09-20T06:24:44.369067Z","iopub.status.idle":"2023-09-20T06:24:44.407212Z","shell.execute_reply.started":"2023-09-20T06:24:44.369039Z","shell.execute_reply":"2023-09-20T06:24:44.405981Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!ls","metadata":{"execution":{"iopub.status.busy":"2023-09-20T06:25:31.759730Z","iopub.execute_input":"2023-09-20T06:25:31.760777Z","iopub.status.idle":"2023-09-20T06:25:32.926973Z","shell.execute_reply.started":"2023-09-20T06:25:31.760732Z","shell.execute_reply":"2023-09-20T06:25:32.925337Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}